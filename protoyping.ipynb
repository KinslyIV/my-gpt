{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c8712a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"datasets/tinyshakespear.txt\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a8a1b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset in chars 1115394\n"
     ]
    }
   ],
   "source": [
    "print(f\"Length of dataset in chars {len(text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917cf24c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8613088d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018799de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
      "hii there\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "tok_to_id = {ch:i for i,ch in enumerate(chars)}\n",
    "id_to_tok = {i:ch for i,ch in enumerate(chars)}\n",
    "# tok_to_id[\"<S>\"] = len(tok_to_id)\n",
    "# tok_to_id[\"<E>\"] = len(tok_to_id)\n",
    "# id_to_tok[len(id_to_tok)] = \"<S>\"\n",
    "# id_to_tok[len(id_to_tok)] = \"<E>\"\n",
    "\n",
    "def encode(s):\n",
    "    if isinstance(s, str) or isinstance(s, list) and isinstance(s[0], str):\n",
    "        return [tok_to_id[c] for c in s]\n",
    "    elif isinstance(s[0], list):\n",
    "        return [encode(ss) for ss in s]\n",
    "    else:\n",
    "        return []\n",
    "    \n",
    "def decode(l):\n",
    "    if isinstance(l, int):\n",
    "        return id_to_tok[l]\n",
    "    elif isinstance(l, list) and isinstance(l[0], int):\n",
    "        return ''.join([id_to_tok[i] for i in l])\n",
    "    elif isinstance(l[0], list):\n",
    "        return [decode(ll) for ll in l]\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "print(encode(\"hii there\"))\n",
    "print(decode(encode(\"hii there\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3087e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "split = int(0.9*len(text))\n",
    "train_set = text[:split]\n",
    "test_set = text[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28c1af0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1003854]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_data = torch.tensor(encode(train_set), dtype=torch.long)\n",
    "test_data = torch.tensor(encode(test_set), dtype=torch.long)\n",
    "CONTEXT_SIZE = 64\n",
    "\n",
    "print(train_data.shape, train_data.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3b1646",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split, batch_size=64):\n",
    "    data = train_data if split == 'train' else test_data\n",
    "    ix = torch.randint(len(data) - CONTEXT_SIZE, (batch_size,))\n",
    "    x = torch.stack([data[i:i+CONTEXT_SIZE] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+CONTEXT_SIZE+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812e4c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(model, eval_iters):\n",
    "    model.eval()\n",
    "    out = {}\n",
    "    for split in ['train', 'test']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for _ in range(eval_iters):\n",
    "            x, y = get_batch(split)\n",
    "            logits = model(x)\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.shape[-1]), y.view(-1))\n",
    "            losses[_] = loss.item()\n",
    "        out[split] = losses.mean().item()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "442774e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "[\"him.\\nThat is renown'd for faith? Be fickle, fortune;\\nFor then, I\", 'e life\\nOf stout Mercutio, and then Tybalt fled;\\nBut by and by co', \"ur master Lucentio.\\n\\nLUCENTIO:\\nTranio, let's go: one thing more \", ' you what services he has done for his country?\\n\\nFirst Citizen:\\n', \"mine honour,\\nI'll point you where you shall have such receiving\\n\", 'ased with the blood of enemies.\\nWhat valiant foemen, like to aut', \"he deliver'd\\nHis gracious pleasure any way therein:\\nBut you, my \", ' to these wars.\\n\\nCOMINIUS:\\nIt is your former promise.\\n\\nMARCIUS:\\n']\n",
      "torch.Size([8, 64])\n",
      "targetas:\n",
      "[\"im.\\nThat is renown'd for faith? Be fickle, fortune;\\nFor then, I \", ' life\\nOf stout Mercutio, and then Tybalt fled;\\nBut by and by com', \"r master Lucentio.\\n\\nLUCENTIO:\\nTranio, let's go: one thing more r\", 'you what services he has done for his country?\\n\\nFirst Citizen:\\nV', \"ine honour,\\nI'll point you where you shall have such receiving\\nA\", 'sed with the blood of enemies.\\nWhat valiant foemen, like to autu', \"e deliver'd\\nHis gracious pleasure any way therein:\\nBut you, my n\", 'to these wars.\\n\\nCOMINIUS:\\nIt is your former promise.\\n\\nMARCIUS:\\nS']\n",
      "torch.Size([8, 64])\n"
     ]
    }
   ],
   "source": [
    "x_batch_test, y_batch_test = get_batch('train', 8)\n",
    "print(\"inputs:\")\n",
    "print(decode(x_batch_test.tolist()))\n",
    "print(x_batch_test.shape)\n",
    "print(\"targetas:\")\n",
    "print(decode(y_batch_test.tolist())) \n",
    "print(y_batch_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a82fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import SimpleModel\n",
    "\n",
    "embed_d = 128\n",
    "n_heads = 4 \n",
    "\n",
    "model = SimpleModel(vocab_size, n_embd=embed_d, context_size=CONTEXT_SIZE, n_heads=n_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0471fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.1710, test loss 4.1712\n",
      "step 200: train loss 2.4924, test loss 2.4900\n",
      "step 400: train loss 2.2409, test loss 2.2579\n",
      "step 600: train loss 2.1127, test loss 2.1607\n",
      "step 800: train loss 2.0226, test loss 2.0991\n",
      "step 1000: train loss 1.9615, test loss 2.0641\n",
      "step 1200: train loss 1.9018, test loss 2.0214\n",
      "step 1400: train loss 1.8589, test loss 2.0000\n",
      "step 1600: train loss 1.8259, test loss 1.9776\n",
      "step 1800: train loss 1.7976, test loss 1.9625\n",
      "step 2000: train loss 1.7693, test loss 1.9426\n",
      "step 2200: train loss 1.7540, test loss 1.9259\n",
      "step 2400: train loss 1.7367, test loss 1.9148\n",
      "step 2600: train loss 1.7217, test loss 1.9097\n",
      "step 2800: train loss 1.7059, test loss 1.8932\n",
      "step 3000: train loss 1.6947, test loss 1.8694\n"
     ]
    }
   ],
   "source": [
    "train_loops = 5000\n",
    "batch_size = 64\n",
    "eval_iters = 200\n",
    "model.to(device)\n",
    "model.train()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "loss = torch.tensor(0.0) \n",
    "\n",
    "for i in range(train_loops):\n",
    "\n",
    "    x_batch, y_batch = get_batch('train', batch_size)\n",
    "\n",
    "    logits = model(x_batch)\n",
    "\n",
    "    B, T, C = logits.shape \n",
    "\n",
    "    logits = logits.view(-1, C)\n",
    "    y_batch = y_batch.view(-1)\n",
    "\n",
    "    # prob = F.log_softmax(logits, dim=-1)\n",
    "    # y_prob = torch.gather(prob, -1, y_batch.unsqueeze(-1)).squeeze()\n",
    "    # loss = -y_prob.mean()\n",
    "  \n",
    "    loss = F.cross_entropy(logits, y_batch)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "   \n",
    "\n",
    "    if i%200 == 0:\n",
    "        losses = estimate_loss(model, eval_iters)\n",
    "        print(f\"step {i}: train loss {losses['train']:.4f}, test loss {losses['test']:.4f}\")\n",
    "\n",
    "losses = estimate_loss(model, eval_iters)\n",
    "print(f\"Final Losses: train loss: {losses['train']}, test loss: {losses['test']} \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "171e7526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(curr_tok)\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(length):\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     next_logits = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurr_tok\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m     \u001b[38;5;66;03m# print(next_logits.shape)\u001b[39;00m\n\u001b[32m      9\u001b[39m     next_prob = F.softmax(next_logits, dim=-\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.openclaw/workspace/my-gpt/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.openclaw/workspace/my-gpt/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.openclaw/workspace/my-gpt/model.py:65\u001b[39m, in \u001b[36mSimpleModel.forward\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[32m     63\u001b[39m     \u001b[38;5;66;03m# idx of shape Batch, Tokens, \u001b[39;00m\n\u001b[32m     64\u001b[39m     tok_emb = \u001b[38;5;28mself\u001b[39m.tok_emb(idx)                                \u001b[38;5;66;03m# shape B, T, n_embd each token in each batch get it's vector embedding\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m     pos_emb = \u001b[38;5;28mself\u001b[39m.pos_emb(torch.arange(\u001b[43midx\u001b[49m\u001b[43m.\u001b[49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \n\u001b[32m     66\u001b[39m                                         device=idx.device))    \u001b[38;5;66;03m# all position embeddings from 0 to T-1, shape T, n_embd\u001b[39;00m\n\u001b[32m     68\u001b[39m     x = pos_emb + tok_emb\n\u001b[32m     69\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.multi_head(x)\n",
      "\u001b[31mIndexError\u001b[39m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "length = 1000\n",
    "\n",
    "curr_tok = torch.randint(vocab_size, (1,)).to(device)\n",
    "print(curr_tok)\n",
    "for i in range(length):\n",
    "    \n",
    "    next_logits = model(curr_tok)\n",
    "    # print(next_logits.shape)\n",
    "    next_prob = F.softmax(next_logits, dim=-1)\n",
    "    # print(next_prob.shape)\n",
    "    next_tok = torch.multinomial(next_prob.squeeze(), num_samples=1)\n",
    "    # print(next_tok)\n",
    "    print(decode(next_tok.tolist()), end=\"\")\n",
    "    curr_tok = next_tok\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e8c1dd91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Batch 0 ===========\n",
      "Given Context: \n",
      "nd, long heath, brown furze, any\n",
      "thing. The wills above be done!\n",
      "\n",
      "Generated Text: \n",
      "\n",
      "e you arat bell! Sarf shore copeat ake rand as yourbest berter.\n",
      "\n",
      "\n",
      "\n",
      "========== Batch 1 ===========\n",
      "Given Context: \n",
      "here deliver them.\n",
      "\n",
      "TRANIO:\n",
      "Well, sir, to do you courtesy,\n",
      "This \n",
      "\n",
      "Generated Text: \n",
      "\n",
      "BELLANT:\n",
      "Sumdrit, a he 'tis.\n",
      "\n",
      "DUKE VINCENTA:\n",
      "Hereaw go your proof\n",
      "\n",
      "========== Batch 2 ===========\n",
      "Given Context: \n",
      "l the honours on my brother: whereon,\n",
      "A treacherous army levied,\n",
      "\n",
      "Generated Text: \n",
      "\n",
      "eend wite 'Woutill might wart to Firs speak as is\n",
      "Acd boosuc.\n",
      "\n",
      "RU\n",
      "\n",
      "========== Batch 3 ===========\n",
      "Given Context: \n",
      "passion, silence! I hear my master.\n",
      "\n",
      "PETRUCHIO:\n",
      "Where be these k\n",
      "\n",
      "Generated Text: \n",
      "\n",
      "o prould\n",
      "And me whis bAy hing ther 'er bout, Andere fore wosen is\n",
      "\n",
      "========== Batch 4 ===========\n",
      "Given Context: \n",
      "ISTA:\n",
      "How likes Gremio these quick-witted folks?\n",
      "\n",
      "GREMIO:\n",
      "Believ\n",
      "\n",
      "Generated Text: \n",
      "\n",
      "Weous be gralk in ford it by lord's becqueen:\n",
      "Welch lions asse an\n",
      "\n",
      "========== Batch 5 ===========\n",
      "Given Context: \n",
      "NDA:\n",
      "There's nothing ill can dwell in such a temple:\n",
      "If the ill \n",
      "\n",
      "Generated Text: \n",
      "\n",
      "ess mopitime,\n",
      "Do II:\n",
      "My comy, and has wropall in ther, for dear e\n",
      "\n",
      "========== Batch 6 ===========\n",
      "Given Context: \n",
      "nca with consent.\n",
      "\n",
      "LUCENTIO:\n",
      "Were it not that my fellow-school-m\n",
      "\n",
      "Generated Text: \n",
      "\n",
      "ar; andly bliss; and be your brother'd lead;\n",
      "And uppon he brith t\n",
      "\n",
      "========== Batch 7 ===========\n",
      "Given Context: \n",
      " estate,\n",
      "An eye-sore to our solemn festival!\n",
      "\n",
      "TRANIO:\n",
      "And tells \n",
      "\n",
      "Generated Text: \n",
      "\n",
      "brett\n",
      "'That paliver, nown nobe suce.\n",
      "Have ward, gurdy thate in to\n",
      "\n"
     ]
    }
   ],
   "source": [
    "length = 1000\n",
    "\n",
    "context_x, targets = get_batch('test', batch_size=8)\n",
    "\n",
    "generated = model.generate(context_x, length)\n",
    "\n",
    "for i, batch in enumerate(generated):\n",
    "    print(f\"========== Batch {i} ===========\")\n",
    "    print(f\"Given Context: \\n{decode(context_x[i].tolist())}\\n\")\n",
    "    text = decode(batch.tolist())\n",
    "    print(\"Generated Text: \\n\")\n",
    "    print(text) \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2413c9b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a11d09f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
