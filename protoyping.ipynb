{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5c8712a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"datasets/tinyshakespear.txt\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "11a8a1b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset in chars 1115394\n"
     ]
    }
   ],
   "source": [
    "print(f\"Length of dataset in chars {len(text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "917cf24c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8613088d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "018799de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
      "hii there\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "tok_to_id = {ch:i for i,ch in enumerate(chars)}\n",
    "id_to_tok = {i:ch for i,ch in enumerate(chars)}\n",
    "# tok_to_id[\"<S>\"] = len(tok_to_id)\n",
    "# tok_to_id[\"<E>\"] = len(tok_to_id)\n",
    "# id_to_tok[len(id_to_tok)] = \"<S>\"\n",
    "# id_to_tok[len(id_to_tok)] = \"<E>\"\n",
    "\n",
    "def encode(s):\n",
    "    if isinstance(s, str) or isinstance(s, list) and isinstance(s[0], str):\n",
    "        return [tok_to_id[c] for c in s]\n",
    "    elif isinstance(s[0], list):\n",
    "        return [encode(ss) for ss in s]\n",
    "    else:\n",
    "        return []\n",
    "    \n",
    "def decode(l):\n",
    "    if isinstance(l, int):\n",
    "        return id_to_tok[l]\n",
    "    elif isinstance(l, list) and isinstance(l[0], int):\n",
    "        return ''.join([id_to_tok[i] for i in l])\n",
    "    elif isinstance(l[0], list):\n",
    "        return [decode(ll) for ll in l]\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "print(encode(\"hii there\"))\n",
    "print(decode(encode(\"hii there\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c3087e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "split = int(0.9*len(text))\n",
    "train_set = text[:split]\n",
    "test_set = text[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "28c1af0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1003854]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_data = torch.tensor(encode(train_set), dtype=torch.long)\n",
    "test_data = torch.tensor(encode(test_set), dtype=torch.long)\n",
    "CONTEXT_SIZE = 64\n",
    "\n",
    "print(train_data.shape, train_data.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "da3b1646",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split, batch_size=64):\n",
    "    data = train_data if split == 'train' else test_data\n",
    "    ix = torch.randint(len(data) - CONTEXT_SIZE, (batch_size,))\n",
    "    x = torch.stack([data[i:i+CONTEXT_SIZE] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+CONTEXT_SIZE+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "812e4c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(model, eval_iters):\n",
    "    model.eval()\n",
    "    out = {}\n",
    "    for split in ['train', 'test']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for _ in range(eval_iters):\n",
    "            x, y = get_batch(split)\n",
    "            logits = model(x)\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.shape[-1]), y.view(-1))\n",
    "            losses[_] = loss.item()\n",
    "        out[split] = losses.mean().item()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "442774e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "[\"ight in your defence:\\nUnsheathe your sword, good father; cry 'Sa\", 'ith\\nthe palsied intercession of such a decayed dotant as\\nyou see', 'd,\\nWith too much riches it confound itself:\\nHad he done so to gr', 'e myself wrong, have I not?\\n\\nSecond Gentleman:\\nYes, that thou ha', 'one may drink, depart,\\nAnd yet partake no venom, for his knowled', ' in the world,\\nHe were as much more villain: you, my lord,\\nDo bu', \"vanity--\\nSo it be new, there's no respect how vile--\\nThat is not\", 'all have\\nyour full time of imprisonment and your deliverance\\nwit']\n",
      "torch.Size([8, 64])\n",
      "targetas:\n",
      "[\"ght in your defence:\\nUnsheathe your sword, good father; cry 'Sai\", 'th\\nthe palsied intercession of such a decayed dotant as\\nyou seem', ',\\nWith too much riches it confound itself:\\nHad he done so to gre', ' myself wrong, have I not?\\n\\nSecond Gentleman:\\nYes, that thou has', 'ne may drink, depart,\\nAnd yet partake no venom, for his knowledg', 'in the world,\\nHe were as much more villain: you, my lord,\\nDo but', \"anity--\\nSo it be new, there's no respect how vile--\\nThat is not \", 'll have\\nyour full time of imprisonment and your deliverance\\nwith']\n",
      "torch.Size([8, 64])\n"
     ]
    }
   ],
   "source": [
    "x_batch_test, y_batch_test = get_batch('train', 8)\n",
    "print(\"inputs:\")\n",
    "print(decode(x_batch_test.tolist()))\n",
    "print(x_batch_test.shape)\n",
    "print(\"targetas:\")\n",
    "print(decode(y_batch_test.tolist())) \n",
    "print(y_batch_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c3a82fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import SimpleModel\n",
    "\n",
    "model = SimpleModel(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0471fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.6311, test loss 4.6395\n",
      "step 200: train loss 2.8955, test loss 2.9079\n",
      "step 400: train loss 2.5567, test loss 2.5759\n",
      "step 600: train loss 2.4976, test loss 2.5214\n",
      "step 800: train loss 2.4821, test loss 2.5037\n",
      "step 1000: train loss 2.4724, test loss 2.4957\n",
      "step 1200: train loss 2.4684, test loss 2.4912\n",
      "step 1400: train loss 2.4660, test loss 2.4907\n",
      "step 1600: train loss 2.4626, test loss 2.4937\n",
      "step 1800: train loss 2.4617, test loss 2.4899\n",
      "step 2000: train loss 2.4597, test loss 2.4867\n",
      "step 2200: train loss 2.4602, test loss 2.4856\n",
      "step 2400: train loss 2.4565, test loss 2.4860\n",
      "step 2600: train loss 2.4576, test loss 2.4872\n",
      "step 2800: train loss 2.4554, test loss 2.4833\n",
      "step 3000: train loss 2.4567, test loss 2.4868\n",
      "step 3200: train loss 2.4577, test loss 2.4865\n",
      "step 3400: train loss 2.4546, test loss 2.4813\n",
      "step 3600: train loss 2.4570, test loss 2.4881\n",
      "step 3800: train loss 2.4551, test loss 2.4873\n",
      "step 4000: train loss 2.4558, test loss 2.4864\n",
      "step 4200: train loss 2.4541, test loss 2.4859\n",
      "step 4400: train loss 2.4549, test loss 2.4862\n",
      "step 4600: train loss 2.4558, test loss 2.4830\n",
      "step 4800: train loss 2.4557, test loss 2.4871\n",
      "Final Loss:  2.477480173110962\n"
     ]
    }
   ],
   "source": [
    "train_loops = 5000\n",
    "batch_size = 64\n",
    "\n",
    "model.to(device)\n",
    "model.train()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.01)\n",
    "loss = torch.tensor(0.0) \n",
    "\n",
    "for i in range(train_loops):\n",
    "\n",
    "    x_batch, y_batch = get_batch('train', batch_size)\n",
    "\n",
    "    logits = model(x_batch)\n",
    "\n",
    "    B, T, C = logits.shape \n",
    "\n",
    "    logits = logits.view(-1, C)\n",
    "    y_batch = y_batch.view(-1)\n",
    "\n",
    "    # prob = F.log_softmax(logits, dim=-1)\n",
    "    # y_prob = torch.gather(prob, -1, y_batch.unsqueeze(-1)).squeeze()\n",
    "    # loss = -y_prob.mean()\n",
    "  \n",
    "    loss = F.cross_entropy(logits, y_batch)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "   \n",
    "\n",
    "    if i%200 == 0:\n",
    "        losses = estimate_loss(model, 200)\n",
    "        print(f\"step {i}: train loss {losses['train']:.4f}, test loss {losses['test']:.4f}\")\n",
    "\n",
    "print(\"Final Loss: \", loss.item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "171e7526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([49], device='cuda:0')\n",
      "sto'TI sa.\n",
      "Ye,\n",
      "\n",
      "ofoyove itound t t ong'this Silineesp'd, rkstoura mourg nto'dove, thanouse thicerell y ORGry, y.\n",
      "G st win e hond routhot'er litimavechind m, t, II th ditepousthe,\n",
      "Fabr tt malls nd thafo!\n",
      "\n",
      "Ty s qulantawik,\n",
      "O:\n",
      "Wherlade I I e wofo fer ss avilld Y:\n",
      "\n",
      "AS:\n",
      "ARDUSTh frad, teveretifigr b wenll llow'n heithte weas wigon thonk vidastigaverwerd quo o we, whessenawhiath oryin m nororeve:\n",
      "\n",
      "ENII blliligut at RDomonon w lo what y'se an w.\n",
      "YO:\n",
      "G w,\n",
      "Wee inatheathis anen into Wietuk heday cig he myondedeint, onewshe Exp sanesuthelvy lindit, th y-wid.\n",
      "COMIUS to!\n",
      "ROUM:\n",
      "LOrauby bore tomed werererot had:\n",
      "Ththe.\n",
      "ARUENoee us pars,\n",
      "INGoinoutar ond h martordanshehargllisboruie\n",
      "TI,\n",
      "S:\n",
      "Rem t l t and borbe gre ibeam ashacke pren sor aft youe g m than y t wer the chal, s allely fe OMENUTrf t RDUKI:\n",
      "\n",
      "d?\n",
      "Fot singe, ct ntt agn,\n",
      "MPUL:\n",
      "offausw d aia top o tre.\n",
      "I:\n",
      "\n",
      "\n",
      "\n",
      "D r mult y w y.\n",
      "Whe bavindof hinins aton oupothestlwalinje atthon y bashes tXFithet'lea y,\n",
      "m.\n",
      "\n",
      "A:\n",
      "I ce, niusertoutouellinchout d.\n",
      "CLLORIEr k b"
     ]
    }
   ],
   "source": [
    "length = 1000\n",
    "\n",
    "curr_tok = torch.randint(vocab_size, (1,)).to(device)\n",
    "print(curr_tok)\n",
    "for i in range(length):\n",
    "    \n",
    "    next_logits = model(curr_tok)\n",
    "    # print(next_logits.shape)\n",
    "    next_prob = F.softmax(next_logits, dim=-1)\n",
    "    # print(next_prob.shape)\n",
    "    next_tok = torch.multinomial(next_prob.squeeze(), num_samples=1)\n",
    "    # print(next_tok)\n",
    "    print(decode(next_tok.tolist()), end=\"\")\n",
    "    curr_tok = next_tok\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c1dd91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-gpt (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
